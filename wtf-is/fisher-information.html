<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Fisher Information</title>
  <link rel="shortcut icon" href="/a9online/static/icons/favicon-pad.png">
  <link rel="stylesheet" href="/a9online/static/reset.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
  <link rel="stylesheet" href="/a9online/static/viewer-style.css" />
  <link rel="stylesheet" href="/a9online/static/content-style.css" />
  <link rel="stylesheet" href="/a9online/static/print-style.css" media="print" />
  <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"></script>
  <script src="/a9online/static/render-math.js"></script>
  <script src="/a9online/static/spoiler.js"></script>
</head>

<body onload="RENDER_MATH.render(document.body);">
  <div id="wrapper">
    <div id="content-frame">
      <div id="content-name">Fisher Information</div>
      <div id="content">
<h1 id="fisher-information">Fisher Information</h1>
<p><strong>Fisher information</strong> (named after <a href="http://en.wikipedia.org/wiki/Ronald_Fisher" target="_blank">Ronald Fisher</a>, who camed up with ANOVA and MLE) measures the amount of information that an observed variable $X$ has about a hidden variable $\theta$. </p>
<p>Let $p(X\mid \theta)$ be the likelihood distribution. Then the log-likelihood is
$$\Mr{LL}(\theta) = \log p(X\mid \theta)$$
We define the <strong>score</strong> as
$$\Mr{score}(\theta) = \fracp{}{\theta} \log p(X\mid \theta)$$
Under regular conditions, the <em>first</em> moment of the score is 0:
$$\begin{aligned} \exx{X}{\Mr{score}(\theta)\mid\theta} &amp;= \int \nail{\fracp{}{\theta} \log p(X\mid \theta)} p(x\mid\theta)\,dx \\ &amp;= \int \nail{\frac{1}{p(x\mid \theta)}\fracp{p(x\mid \theta)}{\theta}} p(x\mid\theta)\,dx \\ &amp;= \fracp{}{\theta} \int p(x\mid\theta)\, dx = \fracp{}{\theta} 1 = 0 \end{aligned}$$</p>
<p>The <strong>Fisher information</strong> is the <em>second</em> moment:
$$I(\theta) = \exx{X}{\nail{\fracp{}{\theta}\log p(X\mid \theta)}^2\midd\theta}$$
Under regular conditions ($p(X\mid \theta)$ is twice differentiable), chain rule will imply 
$$I(\theta) = -\exx{X}{\fracp{^2}{\theta^2}\log p(X\mid \theta)\midd \theta}$$
Note that Fisher information does not depend on a particular $X=x$ since it is integrated out.</p>
<h2 id="intuition">Intuition</h2>
<p>Fisher information measures the <strong>curvature</strong> of the log-likelihood. If we plot the log-likelihood, high curvature = deep valley = easy to get optimal $\theta$ = we got a lot of information about $\theta$ from $X$ = high Fisher information.</p>
<h2 id="properties">Properties</h2>
<ul>
<li>$I_{X,Y}(\theta) = I_X(\theta) + I_Y(\theta)$</li>
<li>If $T(X)$ is a sufficient statistics for $\theta$ (i.e., $$p(X=x\mid T(X)=t,\theta) = p(X=x\mid T(X)=t)$$ independent of $\theta$), then $I_T(\theta) = I_X(\theta)$</li>
<li>For other statistics $T(X)$, we get $I_T(\theta) \leq I_X(\theta)$</li>
<li><strong><em>Theorem</em></strong> (<strong>Cramér-Rao Bound</strong>) For any <em>unbiased</em> estimator $\hat\theta$,
  $$\var{\hat\theta} \geq \frac{1}{I(\theta)}$$
  This makes sense: less information means that it is more difficult to pinpoint the estimator, and thus the variance increases.</li>
<li><strong><em>Definition</em></strong> (<strong>Jeffreys Prior</strong>) The Jeffreys prior is defined as
  $$p_\theta(\theta) \propto \sqrt{I(\theta)}$$
  Jeffreys prior is an uninformative prior that is not sensitive to parameterization; i.e., both the original $p_\theta(\theta)$ and
  $$p_\phi(\phi) = p_\theta(\theta)\abs{\fracd{\theta}{\phi}}$$
  for any reparameterization $\phi = h(\theta)$ will be uninformative.</li>
</ul>
<h1 id="fisher-information-matrix">Fisher Information Matrix</h1>
<p>Suppose we have $N$ parameters $\theta = (\theta_1, \dots, \theta_N)$. Fisher information becomes an $N\times N$ matrix $I(\theta)$ with entries
$$I(\theta)_{ij} = \exx{X}{\nail{\fracp{}{\theta_i} \log p(X\mid \theta)}\nail{\fracp{}{\theta_j} \log p(X\mid \theta)}\midd \theta}$$
Note that $I(\theta) \succeq 0$.</p>
<p>Again, under regular conditions, we also get
$$I(\theta)_{ij} = -\exx{X}{\frac{\partial^2}{\partial \theta_i \partial \theta_j} \log p(X\mid \theta)\midd \theta}$$</p>
<p>If $I(\theta)_{ij} = 0$, we say that $\theta_i$ and $\theta_j$ are <strong>orthogonal parameters</strong>, and their MLE will be independent.</p>
<h1 id="references">References</h1>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Ronald_Fisher#See_also" target="_blank">http://en.wikipedia.org/wiki/Ronald_Fisher#See_also</a></li>
<li><a href="http://en.wikipedia.org/wiki/Fisher_information" target="_blank">http://en.wikipedia.org/wiki/Fisher_information</a></li>
<li><a href="http://mark.reid.name/blog/fisher-information-and-log-likelihood.html" target="_blank">http://mark.reid.name/blog/fisher-information-and-log-likelihood.html</a></li>
<li><a href="http://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound" target="_blank">http://en.wikipedia.org/wiki/Cramér–Rao_bound</a></li>
</ul>

      </div>
      <div id="content-time">Exported: 2021-01-03T09:40:33.660696</div>
    </div>
  </div>
</body>

</html>
